{"cells":[{"cell_type":"markdown","metadata":{"id":"-atAFiq8kLtz"},"source":["# Agressive Social Media Text Processing\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cuSKhlIZe49F"},"source":["## Project Main code\n","1. Read in the dataset\n","1. Pre-process Dataset\n","1. Create Labels for classification\n","1. Build Model 1\n","1. Train Model 1\n","1. Display Accuracy\n","1. Build Model 2\n","1. Train Model 2\n","1. Display Accuracy\n","1. Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"HLl6PmlAlR7f"},"source":["## Prepare Google Drive and Filename"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11875,"status":"ok","timestamp":1665815966980,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"zpaXbQQo6YNu","outputId":"69918163-cacd-420e-89fe-710c8ac56f57"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["#All library imports are here\n","import numpy as np\n","import pandas as pd\n","import re\n","import string\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import keras\n","import tensorflow as tf\n","import tensorflow.python.keras.backend as K\n","from tensorflow.python.client import device_lib\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","import re\n","from keras.layers.core import Dense, Dropout, Activation, Lambda\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are using device name \"cuda\"\n"]}],"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(f'We are using device name \"{device}\"')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":152,"status":"ok","timestamp":1665811225937,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"miF_Qnvmlhrb"},"outputs":[],"source":["dataset_file = 'D:/project/ece570nlp/dataSet.csv'"]},{"cell_type":"markdown","metadata":{"id":"5mvHbC0Jk8_D"},"source":["## Read Dataset\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1665284777040,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"vYXpwD2Eez6l","outputId":"dfe51077-a431-4d07-bd21-59b3f95cc7c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["(12000, 3)\n"]}],"source":["#In this section we Read the dataset from the Paper\n","\n","# Data is stored in csv format\n","\n","# Load CSV using Pandas\n","\n","filename = dataset_file\n","names = ['id', 'post', 'label']\n","df_original = pd.read_csv(filename, names=names, encoding='UTF-8')\n","print(df_original.shape)"]},{"cell_type":"markdown","metadata":{"id":"PTE7O55mnEiG"},"source":["## Print the first 10 lines of the data as an example"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1665284780853,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"jk24zLC2nNDj","outputId":"fefa5b28-e879-4b1e-efb1-07b400dc032b"},"outputs":[{"name":"stdout","output_type":"stream","text":["                            id  \\\n","0   facebook_corpus_msr_401470   \n","1   facebook_corpus_msr_386695   \n","2   facebook_corpus_msr_373389   \n","3   facebook_corpus_msr_917635   \n","4   facebook_corpus_msr_382517   \n","5   facebook_corpus_msr_403274   \n","6  facebook_corpus_msr_1723083   \n","7   facebook_corpus_msr_325257   \n","8    facebook_corpus_msr_23447   \n","9  facebook_corpus_msr_1477104   \n","\n","                                                post label  \n","0  Mahmood Ghaznavi Aor ABdali ko bhol gaya ha tu...   OAG  \n","1  Bhai 60sal pehle desh me kya tha pehle pta kro...   CAG  \n","2  chutiya friday ko isliye releae krte kyoki wee...   CAG  \n","3                                         जय मोदीराज   CAG  \n","4     UPA walo ne bahot kuch kr diya tha desh k liye   CAG  \n","5  Pan ko Aadhar se link ki zarurat kuy hai? Supr...   CAG  \n","6  काकर पाथर जोड़ के मस्जिद लई बनाय।\\n\\nता चढ़ि मुल...   OAG  \n","7  Us raat tere papa k jageh mera sperm gya tha u...   OAG  \n","8                                       गटर के कीड़े   OAG  \n","9  Waise bandhu jet lag se bachne ke liye Raat ko...   NAG  \n"]}],"source":["first_10_rows = df_original.head(10)\n","print(first_10_rows)"]},{"cell_type":"markdown","metadata":{"id":"4X0XWT9l0Izc"},"source":["## Read the processed data back to a dataframe"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"uPehzMc00DsK"},"outputs":[],"source":["dataset_processed_file = 'D:/project/ece570nlp/DataSetProcessed.csv'"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1665285067682,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"FNmIvXYv0RQA","outputId":"98735916-7fef-457f-96c3-ba3802e8b0a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["(11999, 3)\n","                             id  \\\n","0   facebook_corpus_msr_401470    \n","1   facebook_corpus_msr_386695    \n","2   facebook_corpus_msr_373389    \n","3   facebook_corpus_msr_917635    \n","4   facebook_corpus_msr_382517    \n","5   facebook_corpus_msr_403274    \n","6  facebook_corpus_msr_1723083    \n","7   facebook_corpus_msr_325257    \n","8    facebook_corpus_msr_23447    \n","9  facebook_corpus_msr_1477104    \n","\n","                                                post label  \n","0   Mahmood Ghaznavi Aor ABdali ko bhol gaya ha t...   OAG  \n","1   Bhai 60sal pehle desh me kya tha pehle pta kr...   CAG  \n","2   chutiya friday ko isliye releae krte kyoki we...   CAG  \n","3                                       jai modiraj    CAG  \n","4    UPA walo ne bahot kuch kr diya tha desh k liye    CAG  \n","5   Pan ko Aadhar se link ki zarurat kuy hai? Sup...   CAG  \n","6   kaakar pathar jod ke masjid lai banaay. ta ch...   OAG  \n","7   Us raat tere papa k jageh mera sperm gya tha ...   OAG  \n","8                                    gater ke keede    OAG  \n","9   Waise bandhu jet lag se bachne ke liye Raat k...   NAG  \n"]}],"source":["filename = dataset_processed_file\n","header = ['id', 'post', 'label']\n","df_processed = pd.read_csv(filename, names=header)\n","print(df_processed.shape)\n","print(df_processed.head(10))"]},{"cell_type":"markdown","metadata":{"id":"7oIMgR742nGu"},"source":["## Split the dataset into training and test data"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1665285073835,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"w7RKoRuq2wdl","outputId":"d84c8185-8a9b-4b5b-9055-5c11d385c39b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(11999, 2)\n"]}],"source":["df_processed = df_processed.iloc[: , 1:]  # Drop first column\n","print(df_processed.shape)\n","\n","# Save without ID to csv file\n","file_no_id = 'D:/project/ece570nlp/fileNoID.csv'\n","df_processed.to_csv(file_no_id, index=False, header=False)\n","\n","# Decide on a percentage split 70/30\n","#msk = np.random.rand(len(df)) < 0.7\n","#train_df = df_processed[msk]\n","#test_df = df_processed[~msk]\n","\n","#print(train_df.shape)\n","#print(test_df.shape)\n","\n","#print(train_df.head(10))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["file_no_id = 'D:/project/ece570nlp/fileNoID.csv'\n","data_cleanedup = 'D:/project/ece570nlp/datacleanedup.csv'"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(11788, 2)\n","(11788,)\n","(11788,)\n","22.9778236204229\n","11773\n","11773\n","(11773, 387)\n","(11773, 3)\n","(9418, 387)\n","(9418, 3)\n","(2355, 387)\n","(2355, 3)\n"]},{"ename":"NameError","evalue":"name 'device' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\project\\ece570nlp\\project_main_code.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X22sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m X_train_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X22sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m Y_train_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(np\u001b[39m.\u001b[39marray(Y_train))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X22sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m X_train_tensor \u001b[39m=\u001b[39m X_train_tensor\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X22sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m Y_train_tensor \u001b[39m=\u001b[39m Y_train_tensor\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X22sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m X_test_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(X_test)\n","\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"]}],"source":["header = ['post', 'label']\n","df_pretoken = pd.read_csv(data_cleanedup, header=None, names=header)\n","print(df_pretoken.shape)\n","\n","X_pretoken = df_pretoken['post']\n","Y_pretoken = df_pretoken['label']\n","\n","print(X_pretoken.shape)\n","print(Y_pretoken.shape)\n","\n","tokenizer = Tokenizer(num_words=4000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n","tokenizer.fit_on_texts(X_pretoken)\n","#print(tokenizer.word_index)\n","\n","X = tokenizer.texts_to_sequences(X_pretoken)\n","#X = word_tokenize(str(X_pretoken))  # Using NLTK\n","#print(X)\n","\n","\n","sum_tot = 0\n","for seq in X:\n","    sum_tot += len(seq)\n","\n","print(float(sum_tot/11634.0))\n","\n","new_x = []\n","new_y = []\n","\n","for i in range(0, len(X)):\n","    if len(X[i]) < 400:\n","        new_x.append(X[i])\n","        new_y.append(Y_pretoken[i])\n","\n","print(len(new_x))\n","print(len(new_y))\n","\n","X = pad_sequences(new_x)\n","Y = pd.get_dummies(new_y)\n","\n","print(X.shape)\n","print(Y.shape)\n","\n","#print(X)\n","#print(Y)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n","\n","print(X_train.shape)\n","print(Y_train.shape)\n","print(X_test.shape)\n","print(Y_test.shape)\n","\n","\n","X_train_tensor = torch.Tensor(X_train)\n","Y_train_tensor = torch.Tensor(np.array(Y_train))\n","\n","X_train_tensor = X_train_tensor.to(device)\n","Y_train_tensor = Y_train_tensor.to(device)\n","\n","X_test_tensor = torch.Tensor(X_test)\n","Y_test_tensor = torch.Tensor(np.array(Y_test))\n","\n","X_test_tensor = X_test_tensor.to(device)\n","Y_test_tensor = Y_test_tensor.to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"jhr0C61ariHV"},"source":["Build the MLP Network"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7663,"status":"ok","timestamp":1665816043228,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"JOxsNpLvKVmA","outputId":"3e339604-f3ca-426b-a2eb-2351c239c69f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 8723625169893544401\n","xla_global_id: -1\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 10043260928\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 6798829934341024794\n","physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n","xla_global_id: 416903419\n","]\n","Num GPUs Available:  1\n","Training...\n","Epoch 1/5\n","236/236 [==============================] - 3s 4ms/step - loss: 49.3329 - accuracy: 0.3809 - val_loss: 1.8381 - val_accuracy: 0.3848\n","Epoch 2/5\n","236/236 [==============================] - 1s 3ms/step - loss: 2.2698 - accuracy: 0.3945 - val_loss: 1.0719 - val_accuracy: 0.4071\n","Epoch 3/5\n","236/236 [==============================] - 1s 3ms/step - loss: 1.3235 - accuracy: 0.3973 - val_loss: 1.0547 - val_accuracy: 0.4071\n","Epoch 4/5\n","236/236 [==============================] - 1s 3ms/step - loss: 1.1637 - accuracy: 0.4012 - val_loss: 1.0482 - val_accuracy: 0.4066\n","Epoch 5/5\n","236/236 [==============================] - 1s 3ms/step - loss: 1.1633 - accuracy: 0.4063 - val_loss: 1.0468 - val_accuracy: 0.4071\n","Generating test predictions...\n","[[0.39787236 0.20622529 0.39590237]\n"," [0.39787236 0.20622529 0.39590237]\n"," [0.39787236 0.20622529 0.39590237]\n"," ...\n"," [0.39787236 0.20622529 0.39590237]\n"," [0.39787236 0.20622529 0.39590237]\n"," [0.39787236 0.20622529 0.39590237]]\n","74/74 - 0s - loss: 1.0502 - accuracy: 0.4149 - 101ms/epoch - 1ms/step\n","Score: 1.05\n","Validation Accuracy: 0.41\n"]}],"source":["\n","print(device_lib.list_local_devices())\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","\n","# Deep Neural Network \"MLP\": multi layer Perceptron\n","#with tf.device('/GPU:0'):\n","model = Sequential()\n","\n","\n","model.add(Dense(256, input_dim=X_train.shape[1]))\n","\n","# 0.42 accuracy.\n","model.add(Activation('relu'))\n","model.add(Dropout(0.4))\n","model.add(Dense(128))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(3))\n","model.add(Activation('softmax'))\n","\n","# we'll use categorical xent for the loss, and RMSprop as the optimizer\n","model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n","\n","print(\"Training...\")\n","model.fit(X_train, Y_train, epochs=5, batch_size=32, validation_split=0.2)\n","\n","print(\"Generating test predictions...\")\n","preds = model.predict(X_test, verbose=0)\n","print(preds)\n","\n","# Evaluating the model\n","score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = 32)\n","print(\"Score: %.2f\" % (score))\n","print(\"Validation Accuracy: %.2f\" % (acc))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are using device name \"cuda\"\n"]}],"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(f'We are using device name \"{device}\"')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":363,"status":"ok","timestamp":1665816049410,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"VUc6d02pPnfi","outputId":"528f0578-d32b-4313-d541-110b750d95b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["74/74 [==============================] - 0s 2ms/step\n"]}],"source":["predict = model.predict(X_test)\n","preds = predict\n","p = preds"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":493,"status":"error","timestamp":1665816052127,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"SCDeuT03Ppfg","outputId":"960a3941-59fb-4162-cd46-eaa2cbfd3765"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.564612612222589\n","0.4148619957537155\n","0.24378579952887636\n","0.7573939285707842\n","0.4148619957537155\n","0.2437861012494078\n","0.6516020032365524\n","0.8072186836518047\n","0.7211102996344364\n","0.7616524793142448\n","0.6076433121019108\n","0.45979269369900566\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n","#print(tmpMat.shape)\n","for i in range(0,len(predict)):\n","    if(p[i][0] > p[i][1] and p[i][0] > p[i][2]):\n","        tmpMat[i][0]=1\n","    elif(p[i][1] > p[i][0] and p[i][1] > p[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","\n","test_y = Y_test.to_numpy()\n","\n","\n","\n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))  \n","\n","\n","\n","# OF CAG\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","\n","# Of class NAG\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","# Of class OAG\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1665816067483,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"B_KshrOfHNhH"},"outputs":[],"source":["#CNN\n","# CNN: ConvNeuralNets\n","\n","nb_filter = 250\n","filter_length = 3\n","hidden_dims = 250\n","nb_epoch = 2"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1078,"status":"ok","timestamp":1665816071960,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"J_TZyIBeHV5z","outputId":"a692dfda-e412-4cd7-b452-b3a48190a955"},"outputs":[{"name":"stdout","output_type":"stream","text":["Build model...\n"]}],"source":["from keras.layers.convolutional import Convolution1D\n","from keras import backend as K\n","\n","print('Build model...')\n","model = Sequential()\n","model.add(Embedding(4000, 128))\n","# we add a Convolution1D, which will learn nb_filter\n","# word group filters of size filter_length:\n","model.add(Convolution1D(4000, 128, \n","                        activation='relu'))\n","\n","def max_1d(X_train):\n","    return K.max(X_train, axis=1)\n","\n","model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n","model.add(Dense(hidden_dims)) \n","model.add(Dropout(0.2)) \n","model.add(Activation('relu'))\n","model.add(Dense(3))\n","model.add(Activation('sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNmO92j-Hin_","outputId":"d3287bda-97d9-4b23-aa2d-60575b95627b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train...\n","Epoch 1/2\n","76/76 [==============================] - 211s 2s/step - loss: 0.6398 - accuracy: 0.4208 - val_loss: 0.5740 - val_accuracy: 0.4931\n","Epoch 2/2\n","76/76 [==============================] - 98s 1s/step - loss: 0.5242 - accuracy: 0.5668 - val_loss: 0.5361 - val_accuracy: 0.5732\n","74/74 [==============================] - 8s 101ms/step - loss: 0.5240 - accuracy: 0.5851\n","Test score: 0.5239724516868591\n","Test accuracy: 0.5851380228996277\n"]}],"source":["print('Train...')\n","model.fit(X_train, Y_train, batch_size=100, epochs=2,\n","          validation_split=0.2)\n","score, acc = model.evaluate(X_test, Y_test, batch_size=32)\n","print('Test score:', score)\n","print('Test accuracy:', acc)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"k-tugF4iQXjc"},"outputs":[{"name":"stdout","output_type":"stream","text":["74/74 [==============================] - 7s 93ms/step\n","0.6184213902166585\n","0.5851380042462845\n","0.5766898025638827\n","0.6497355892414785\n","0.6067940552016985\n","0.6053642154024319\n","0.8292502781534452\n","0.8450106157112527\n","0.8230072559993208\n","0.7140650532376177\n","0.7184713375796178\n","0.7096931295772451\n"]}],"source":["predict = model.predict(X_test)\n","test_y = Y_test.to_numpy()\n","tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n","\n","for i in range(0,len(predict)):\n","    if(predict[i][0] > predict[i][1] and predict[i][0] > predict[i][2]):\n","        tmpMat[i][0]=1\n","    elif(predict[i][1] > predict[i][0] and predict[i][1] > predict[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","        \n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))\n","\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"EB6CYs8PQNs-"},"source":["## LSTM"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"qhPwhGFqQQwd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 387, 128)          512000    \n","                                                                 \n"," lstm (LSTM)                 (None, 196)               254800    \n","                                                                 \n"," dense_5 (Dense)             (None, 3)                 591       \n","                                                                 \n","=================================================================\n","Total params: 767,391\n","Trainable params: 767,391\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["# LSTM Model for prediction\n","embed_dim = 128\n","lstm_out = 196\n","batch_size = 100\n","\n","model = Sequential()\n","#model.add(Embedding(4000, embed_dim,input_length = X.shape[1], dropout = 0.2))\n","model.add(Embedding(4000, embed_dim,input_length = X.shape[1]))\n","#model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n","model.add(LSTM(lstm_out))\n","model.add(Dense(3,activation='softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","print(model.summary())"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"_hSqeA77QrI1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","95/95 - 4s - loss: 0.9726 - accuracy: 0.4816 - 4s/epoch - 41ms/step\n","Epoch 2/2\n","95/95 - 2s - loss: 0.7769 - accuracy: 0.6350 - 2s/epoch - 23ms/step\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1f4c42fcaf0>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(X_train, Y_train, batch_size = batch_size, epochs = 2, verbose = 2)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"_Z7MmCfNQtRB"},"outputs":[{"name":"stdout","output_type":"stream","text":["24/24 - 1s - loss: 0.8816 - accuracy: 0.5843 - 661ms/epoch - 28ms/step\n","Score: 0.88\n","Validation Accuracy: 0.58\n"]}],"source":["score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n","print(\"Score: %.2f\" % (score))\n","print(\"Validation Accuracy: %.2f\" % (acc))"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"a6iyQPUAQvyV"},"outputs":[{"name":"stdout","output_type":"stream","text":["74/74 [==============================] - 1s 12ms/step\n"]}],"source":["p = model.predict(X_test)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"dQu2GzEPQyLW"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5917995077059791\n","0.5842887473460722\n","0.5846419471518496\n","0.6486808198223001\n","0.640764331210191\n","0.6430169694082171\n","0.8265756703758201\n","0.8195329087048833\n","0.8227128891568494\n","0.7029169958869594\n","0.70828025477707\n","0.7018540430125413\n"]}],"source":["tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n","\n","for i in range(0,len(predict)):\n","    if(p[i][0] > p[i][1] and p[i][0] > p[i][2]):\n","        tmpMat[i][0]=1\n","    elif(p[i][1] > p[i][0] and p[i][1] > p[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","\n","test_y = Y_test.to_numpy()\n","\n","# For LSTM model\n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))\n","\n","# Of Class CAG\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","# Of class NAG\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","# Of class OAG\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"ACpaROBsRBJv"},"source":["## Naive Bayes"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"8x1p8S5mQ-iV"},"outputs":[{"name":"stdout","output_type":"stream","text":["- CE : 4.24, KL : 0.46\n"]}],"source":["import numpy as np\n","from sklearn import datasets\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchbnn as bnn\n","\n","model = nn.Sequential(\n","    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=X_train.shape[1], out_features=4000),\n","    nn.ReLU(),\n","    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=4000, out_features=3),\n",")\n","\n","model = model.to(device)\n","\n","\n","ce_loss = nn.CrossEntropyLoss()\n","kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n","kl_weight = 0.01\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","kl_weight = 0.1\n","\n","for step in range(3000):\n","    pre = model(X_train_tensor)\n","    ce = ce_loss(pre, Y_train_tensor)\n","    kl = kl_loss(model)\n","    cost = ce + kl_weight*kl\n","    \n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","    \n","_, predicted = torch.max(pre.data, 1)\n","total = Y_train_tensor.size(0)\n","#correct = (predicted == Y_train_tensor).sum()\n","#print('- Accuracy: %f %%' % (100 * float(correct) / total))\n","print('- CE : %2.2f, KL : %2.2f' % (ce.item(), kl.item()))\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["model.eval()\n","predict = model(X_test_tensor)\n","preds = predict\n","p = preds"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.4150505020856367\n","0.4220806794055202\n","0.3529458830358359\n","0.5447077314024311\n","0.4653927813163482\n","0.41799338117676954\n","0.7518767875744102\n","0.7987261146496816\n","0.7594931836701142\n","0.5225647022187301\n","0.5800424628450106\n","0.5097267398257043\n"]}],"source":["tmpMat = np.zeros((len(Y_test_tensor), 3), dtype=int)\n","#print(tmpMat.shape)\n","for i in range(0,len(predict)):\n","    if(p[i][0] > p[i][1] and p[i][0] > p[i][2]):\n","        tmpMat[i][0]=1\n","    elif(p[i][1] > p[i][0] and p[i][1] > p[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","\n","\n","Y_test_tensor = Y_test_tensor.cpu().detach()\n","test_y = Y_test_tensor.numpy()\n","\n","\n","\n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))  \n","\n","\n","\n","# OF CAG\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","\n","# Of class NAG\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","# Of class OAG\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"_kkKQo9mRKgS"},"source":["## SVM"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(8202, 2)\n","(3586, 2)\n","(8202, 4302)\n"]}],"source":["#Import svm model\n","from sklearn import svm\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Decide on a percentage split 70/30\n","msk = np.random.rand(len(df_pretoken)) < 0.7\n","train_df = df_pretoken[msk]\n","test_df = df_pretoken[~msk]\n","\n","print(train_df.shape)\n","print(test_df.shape)\n","\n","#print(train_df.head(10))\n","\n","vectorizer = TfidfVectorizer(min_df=5, max_df=0.8, \n","                            sublinear_tf=True, use_idf=True)\n","train_features = vectorizer.fit_transform(train_df['post'])\n","test_features = vectorizer.transform(test_df['post'])\n","print(train_features.shape)\n","\n","#Create a svm Classifier\n","model = svm.SVC(kernel='linear') # Linear Kernel\n","\n","#Train the model using the training sets\n","model.fit(train_features, train_df['label'])\n","\n","#Predict the response for test dataset\n","predict = model.predict(test_features)"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["preds = predict\n","p = preds"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.5755716675962075\n"]}],"source":["#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics\n","\n","# Model Accuracy: how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(test_df['label'],predict))"]},{"cell_type":"markdown","metadata":{"id":"obkby72nRUXY"},"source":["## Transfer Learning"]},{"cell_type":"markdown","metadata":{},"source":["### Get the indic bert transformer"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"8383SyYqTL9r"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'sop_classifier.classifier.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'sop_classifier.classifier.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoModel, AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n","model = AutoModel.from_pretrained('ai4bharat/indic-bert')"]},{"cell_type":"markdown","metadata":{},"source":["### Customize the Transformer for the required input and output"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AlbertModel(\n","  (embeddings): AlbertEmbeddings(\n","    (word_embeddings): Embedding(4000, 128, padding_idx=0)\n","    (position_embeddings): Embedding(512, 128)\n","    (token_type_embeddings): Embedding(2, 128)\n","    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0, inplace=False)\n","  )\n","  (encoder): AlbertTransformer(\n","    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n","    (albert_layer_groups): ModuleList(\n","      (0): AlbertLayerGroup(\n","        (albert_layers): ModuleList(\n","          (0): AlbertLayer(\n","            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (attention): AlbertAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (attention_dropout): Dropout(p=0, inplace=False)\n","              (output_dropout): Dropout(p=0, inplace=False)\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n","            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (pooler): Linear(in_features=768, out_features=3, bias=True)\n","  (pooler_activation): Tanh()\n",")\n"]}],"source":["model.embeddings.word_embeddings = nn.Embedding(4000,128,padding_idx=0)\n","model.pooler = nn.Linear(in_features = 768, out_features=3)\n","\n","print(model)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Display a few lines of the dataset"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["<bound method NDFrame.head of                                                     post  label\n","0       Mahmood Ghaznavi Aor ABdali ko bhol gaya ha t...      1\n","1       Bhai 60sal pehle desh me kya tha pehle pta kr...      0\n","2       chutiya friday ko isliye releae krte kyoki we...      0\n","3                                           jai modiraj       0\n","4        UPA walo ne bahot kuch kr diya tha desh k liye       0\n","...                                                  ...    ...\n","11783       Pak congress ne hi di hui den hai duniya ko       0\n","11784   or shart yah hogi ki un fiter jet plance kaa ...      1\n","11785   abhi thodi der main grahamantri ji kadi ninda...      0\n","11786   mai bas main baitha tha  mere bagal main ek a...      1\n","11787   Aziz Hala thodi si chook kar di tuune  ajij h...      1\n","\n","[11788 rows x 2 columns]>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["df_pretoken.head"]},{"cell_type":"markdown","metadata":{},"source":["### Split the training and testing datasets\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<bound method NDFrame.head of                                                     post  label\n","0       Mahmood Ghaznavi Aor ABdali ko bhol gaya ha t...      1\n","1       Bhai 60sal pehle desh me kya tha pehle pta kr...      0\n","2       chutiya friday ko isliye releae krte kyoki we...      0\n","3                                           jai modiraj       0\n","4        UPA walo ne bahot kuch kr diya tha desh k liye       0\n","...                                                  ...    ...\n","11783       Pak congress ne hi di hui den hai duniya ko       0\n","11784   or shart yah hogi ki un fiter jet plance kaa ...      1\n","11785   abhi thodi der main grahamantri ji kadi ninda...      0\n","11786   mai bas main baitha tha  mere bagal main ek a...      1\n","11787   Aziz Hala thodi si chook kar di tuune  ajij h...      1\n","\n","[11788 rows x 2 columns]>\n"]}],"source":["# split train dataset into train, validation and test sets\n","df_pretoken['label'] = df_pretoken['label'].replace([\" CAG\",\" OAG\",\" NAG\"],[0,1,2]) \n","train_text, temp_text, train_labels, temp_labels = train_test_split(df_pretoken['post'], df_pretoken['label'], random_state=2018, test_size=0.3, stratify=df_pretoken['label'])\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, random_state=2018, test_size=0.5, stratify=temp_labels)\n","\n","print(df_pretoken.head)"]},{"cell_type":"markdown","metadata":{},"source":["### Check the Bert Model"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [[2, 255, 54, 33, 8, 121763, 10903, 55053, 116316, 3, 0], [2, 491, 168, 15527, 30, 197789, 33, 8, 121763, 10903, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["# sample data\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n","\n","# encode text\n","sent_id = tokenizer.batch_encode_plus(text, padding=True)\n","\n","# output\n","print(sent_id)"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenize the sentences"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["<AxesSubplot:>"]},"execution_count":34,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins = 30)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = 25,\n","    padding=True,\n","    truncation=True\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = 25,\n","    padding=True,\n","    truncation=True\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = 25,\n","    padding=True,\n","    truncation=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Convert list to tensors"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["## convert lists to tensors\n","\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n"]},{"cell_type":"markdown","metadata":{},"source":["### Create Training and validation Set"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["### Define model architecture"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["200000\n"]}],"source":["# freeze all the parameters\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# push the model to GPU\n","#model = model.to(device)\n","\n","print(model.config.vocab_size)"]},{"cell_type":"markdown","metadata":{},"source":["### Bert model"]},{"cell_type":"markdown","metadata":{},"source":["### Pass the model"]},{"cell_type":"markdown","metadata":{},"source":["### Optimizer"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# optimizer from hugging face transformers\n","from torch.optim import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),\n","                  lr = 1e-5)          # learning rate"]},{"cell_type":"markdown","metadata":{},"source":["### Class implementation"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Class Weights: [0.82124017 0.82369971 1.75965025]\n"]}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","\n","\n","class_weights = compute_class_weight(\n","                                        class_weight = \"balanced\",\n","                                        classes = np.unique(train_labels),\n","                                        y = train_labels                                                    \n","                                    )\n","\n","\n","\n","print(\"Class Weights:\",class_weights)"]},{"cell_type":"markdown","metadata":{},"source":["### Class weight to tensor"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["# converting list of class weights to a tensor\n","weights= torch.tensor(class_weights,dtype=torch.float)\n","\n","# push to GPU\n","#weights = weights.to(device)\n","\n","# define the loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","# number of training epochs\n","epochs = 10"]},{"cell_type":"markdown","metadata":{},"source":["### Fine tune indic bert"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# function to train the model\n","def train():\n","  \n","  model.train()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save model predictions\n","  total_preds=[]\n","  \n","  # iterate over batches\n","  for step,batch in enumerate(train_dataloader):\n","    \n","    # progress update after every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","    # push the batch to gpu\n","    #batch = [r.to(device) for r in batch]\n","    #batch = [for r in batch]\n"," \n","    sent_id, mask, labels = batch\n","\n","    # clear previously calculated gradients \n","    model.zero_grad()        \n","\n","    # get model predictions for the current batch\n","    preds = model(sent_id, mask)\n","\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(preds, labels)\n","\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # update parameters\n","    optimizer.step()\n","\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  #returns the loss and predictions\n","  return avg_loss, total_preds"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation Function"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["from datetime import date, datetime, time\n","from babel.dates import format_date, format_datetime, format_time\n","\n","# function for evaluating the model\n","def evaluate():\n","  \n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save the model predictions\n","  total_preds = []\n","\n","  # iterate over batches\n","  for step,batch in enumerate(val_dataloader):\n","    \n","    # Progress update every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      \n","         \n","      # Report progress.\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","    # push the batch to gpu\n","    #batch = [t.to(device) for t in batch]\n","    #batch = [t for t in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","      \n","      # model predictions\n","      preds = model(sent_id, mask)\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(preds,labels)\n","\n","      total_loss = total_loss + loss.item()\n","\n","      preds = preds.detach().cpu().numpy()\n","\n","      total_preds.append(preds)\n","\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader) \n","\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  return avg_loss, total_preds"]},{"cell_type":"markdown","metadata":{},"source":["### Start fine tuning bert model"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Epoch 1 / 2\n"]},{"ename":"IndexError","evalue":"index out of range in self","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32md:\\project\\ece570nlp\\project_main_code.ipynb Cell 74\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, epochs))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train_loss, _ \u001b[39m=\u001b[39m train()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#evaluate model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m valid_loss, _ \u001b[39m=\u001b[39m evaluate()\n","\u001b[1;32md:\\project\\ece570nlp\\project_main_code.ipynb Cell 74\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()        \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# get model predictions for the current batch\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m preds \u001b[39m=\u001b[39m model(sent_id, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# compute the loss between actual and predicted values\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#Y133sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy(preds, labels)\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:730\u001b[0m, in \u001b[0;36mAlbertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    727\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m extended_attention_mask) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mfinfo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin\n\u001b[0;32m    728\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 730\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m    731\u001b[0m     input_ids, position_ids\u001b[39m=\u001b[39;49mposition_ids, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids, inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds\n\u001b[0;32m    732\u001b[0m )\n\u001b[0;32m    733\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m    734\u001b[0m     embedding_output,\n\u001b[0;32m    735\u001b[0m     extended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    740\u001b[0m )\n\u001b[0;32m    742\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:249\u001b[0m, in \u001b[0;36mAlbertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    246\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    248\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[0;32m    250\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    252\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n","\u001b[1;31mIndexError\u001b[0m: index out of range in self"]}],"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","epochs = 2\n","\n","#for each epoch\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOeXCBAyBTSGoUfgw1A6UT6","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"5152cc3a395a70098988def549eacca6eb731ce4ed08ed350a66e5f78a6f87ea"}}},"nbformat":4,"nbformat_minor":0}
