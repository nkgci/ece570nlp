{"cells":[{"cell_type":"markdown","metadata":{"id":"-atAFiq8kLtz"},"source":["# Agressive Social Media Text Processing\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cuSKhlIZe49F"},"source":["## Project Main code\n","1. Read in the dataset\n","1. Pre-process Dataset\n","1. Create Labels for classification\n","1. Build Model 1\n","1. Train Model 1\n","1. Display Accuracy\n","1. Build Model 2\n","1. Train Model 2\n","1. Display Accuracy\n","1. Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"HLl6PmlAlR7f"},"source":["## Prepare Google Drive and Filename"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11875,"status":"ok","timestamp":1665815966980,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"zpaXbQQo6YNu","outputId":"69918163-cacd-420e-89fe-710c8ac56f57"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["#All library imports are here\n","import numpy as np\n","import pandas as pd\n","import re\n","import string\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import keras\n","import tensorflow as tf\n","import tensorflow.python.keras.backend as K\n","from tensorflow.python.client import device_lib\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","import re\n","from keras.layers.core import Dense, Dropout, Activation, Lambda\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are using device name \"cuda\"\n"]}],"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(f'We are using device name \"{device}\"')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":152,"status":"ok","timestamp":1665811225937,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"miF_Qnvmlhrb"},"outputs":[],"source":["dataset_file = 'D:/project/ece570nlp/dataSet.csv'"]},{"cell_type":"markdown","metadata":{"id":"5mvHbC0Jk8_D"},"source":["## Read Dataset\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1665284777040,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"vYXpwD2Eez6l","outputId":"dfe51077-a431-4d07-bd21-59b3f95cc7c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["(12000, 3)\n"]}],"source":["#In this section we Read the dataset from the Paper\n","\n","# Data is stored in csv format\n","\n","# Load CSV using Pandas\n","\n","filename = dataset_file\n","names = ['id', 'post', 'label']\n","df_original = pd.read_csv(filename, names=names, encoding='UTF-8')\n","print(df_original.shape)"]},{"cell_type":"markdown","metadata":{"id":"PTE7O55mnEiG"},"source":["## Print the first 10 lines of the data as an example"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1665284780853,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"jk24zLC2nNDj","outputId":"fefa5b28-e879-4b1e-efb1-07b400dc032b"},"outputs":[{"name":"stdout","output_type":"stream","text":["                            id  \\\n","0   facebook_corpus_msr_401470   \n","1   facebook_corpus_msr_386695   \n","2   facebook_corpus_msr_373389   \n","3   facebook_corpus_msr_917635   \n","4   facebook_corpus_msr_382517   \n","5   facebook_corpus_msr_403274   \n","6  facebook_corpus_msr_1723083   \n","7   facebook_corpus_msr_325257   \n","8    facebook_corpus_msr_23447   \n","9  facebook_corpus_msr_1477104   \n","\n","                                                post label  \n","0  Mahmood Ghaznavi Aor ABdali ko bhol gaya ha tu...   OAG  \n","1  Bhai 60sal pehle desh me kya tha pehle pta kro...   CAG  \n","2  chutiya friday ko isliye releae krte kyoki wee...   CAG  \n","3                                         जय मोदीराज   CAG  \n","4     UPA walo ne bahot kuch kr diya tha desh k liye   CAG  \n","5  Pan ko Aadhar se link ki zarurat kuy hai? Supr...   CAG  \n","6  काकर पाथर जोड़ के मस्जिद लई बनाय।\\n\\nता चढ़ि मुल...   OAG  \n","7  Us raat tere papa k jageh mera sperm gya tha u...   OAG  \n","8                                       गटर के कीड़े   OAG  \n","9  Waise bandhu jet lag se bachne ke liye Raat ko...   NAG  \n"]}],"source":["first_10_rows = df_original.head(10)\n","print(first_10_rows)"]},{"cell_type":"markdown","metadata":{"id":"4X0XWT9l0Izc"},"source":["## Read the processed data back to a dataframe"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"uPehzMc00DsK"},"outputs":[],"source":["dataset_processed_file = 'D:/project/ece570nlp/DataSetProcessed.csv'"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1665285067682,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"FNmIvXYv0RQA","outputId":"98735916-7fef-457f-96c3-ba3802e8b0a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["(11999, 3)\n","                             id  \\\n","0   facebook_corpus_msr_401470    \n","1   facebook_corpus_msr_386695    \n","2   facebook_corpus_msr_373389    \n","3   facebook_corpus_msr_917635    \n","4   facebook_corpus_msr_382517    \n","5   facebook_corpus_msr_403274    \n","6  facebook_corpus_msr_1723083    \n","7   facebook_corpus_msr_325257    \n","8    facebook_corpus_msr_23447    \n","9  facebook_corpus_msr_1477104    \n","\n","                                                post label  \n","0   Mahmood Ghaznavi Aor ABdali ko bhol gaya ha t...   OAG  \n","1   Bhai 60sal pehle desh me kya tha pehle pta kr...   CAG  \n","2   chutiya friday ko isliye releae krte kyoki we...   CAG  \n","3                                       jai modiraj    CAG  \n","4    UPA walo ne bahot kuch kr diya tha desh k liye    CAG  \n","5   Pan ko Aadhar se link ki zarurat kuy hai? Sup...   CAG  \n","6   kaakar pathar jod ke masjid lai banaay. ta ch...   OAG  \n","7   Us raat tere papa k jageh mera sperm gya tha ...   OAG  \n","8                                    gater ke keede    OAG  \n","9   Waise bandhu jet lag se bachne ke liye Raat k...   NAG  \n"]}],"source":["filename = dataset_processed_file\n","header = ['id', 'post', 'label']\n","df_processed = pd.read_csv(filename, names=header)\n","print(df_processed.shape)\n","print(df_processed.head(10))"]},{"cell_type":"markdown","metadata":{"id":"7oIMgR742nGu"},"source":["## Split the dataset into training and test data"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1665285073835,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"w7RKoRuq2wdl","outputId":"d84c8185-8a9b-4b5b-9055-5c11d385c39b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(11999, 2)\n"]}],"source":["df_processed = df_processed.iloc[: , 1:]  # Drop first column\n","print(df_processed.shape)\n","\n","# Save without ID to csv file\n","file_no_id = 'D:/project/ece570nlp/fileNoID.csv'\n","df_processed.to_csv(file_no_id, index=False, header=False)\n","\n","# Decide on a percentage split 70/30\n","#msk = np.random.rand(len(df)) < 0.7\n","#train_df = df_processed[msk]\n","#test_df = df_processed[~msk]\n","\n","#print(train_df.shape)\n","#print(test_df.shape)\n","\n","#print(train_df.head(10))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["file_no_id = 'D:/project/ece570nlp/fileNoID.csv'\n","data_cleanedup = 'D:/project/ece570nlp/datacleanedup.csv'"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(11788, 2)\n","(11788,)\n","(11788,)\n","22.9778236204229\n","11773\n","11773\n","(11773, 387)\n","(11773, 3)\n","(9418, 387)\n","(9418, 3)\n","(2355, 387)\n","(2355, 3)\n"]}],"source":["header = ['post', 'label']\n","df_pretoken = pd.read_csv(data_cleanedup, header=None, names=header)\n","print(df_pretoken.shape)\n","\n","X_pretoken = df_pretoken['post']\n","Y_pretoken = df_pretoken['label']\n","\n","print(X_pretoken.shape)\n","print(Y_pretoken.shape)\n","\n","tokenizer = Tokenizer(num_words=4000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n","tokenizer.fit_on_texts(X_pretoken)\n","#print(tokenizer.word_index)\n","\n","X = tokenizer.texts_to_sequences(X_pretoken)\n","#X = word_tokenize(str(X_pretoken))  # Using NLTK\n","#print(X)\n","\n","\n","sum_tot = 0\n","for seq in X:\n","    sum_tot += len(seq)\n","\n","print(float(sum_tot/11634.0))\n","\n","new_x = []\n","new_y = []\n","\n","for i in range(0, len(X)):\n","    if len(X[i]) < 400:\n","        new_x.append(X[i])\n","        new_y.append(Y_pretoken[i])\n","\n","print(len(new_x))\n","print(len(new_y))\n","\n","X = pad_sequences(new_x)\n","Y = pd.get_dummies(new_y)\n","\n","print(X.shape)\n","print(Y.shape)\n","\n","#print(X)\n","#print(Y)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n","\n","print(X_train.shape)\n","print(Y_train.shape)\n","print(X_test.shape)\n","print(Y_test.shape)\n","\n","\n","X_train_tensor = torch.Tensor(X_train)\n","Y_train_tensor = torch.Tensor(np.array(Y_train))\n","\n","X_train_tensor = X_train_tensor.to(device)\n","Y_train_tensor = Y_train_tensor.to(device)\n","\n","X_test_tensor = torch.Tensor(X_test)\n","Y_test_tensor = torch.Tensor(np.array(Y_test))\n","\n","X_test_tensor = X_test_tensor.to(device)\n","Y_test_tensor = Y_test_tensor.to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"jhr0C61ariHV"},"source":["Build the MLP Network"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7663,"status":"ok","timestamp":1665816043228,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"JOxsNpLvKVmA","outputId":"3e339604-f3ca-426b-a2eb-2351c239c69f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 14460300535861502525\n","xla_global_id: -1\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 10043260928\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 1685448920583305316\n","physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n","xla_global_id: 416903419\n","]\n","Num GPUs Available:  1\n","Training...\n","Epoch 1/5\n","236/236 [==============================] - 3s 3ms/step - loss: 53.0205 - accuracy: 0.3886 - val_loss: 3.4484 - val_accuracy: 0.4252\n","Epoch 2/5\n","236/236 [==============================] - 1s 2ms/step - loss: 4.1388 - accuracy: 0.3857 - val_loss: 1.0743 - val_accuracy: 0.4135\n","Epoch 3/5\n","236/236 [==============================] - 0s 2ms/step - loss: 1.4926 - accuracy: 0.4056 - val_loss: 1.0586 - val_accuracy: 0.4092\n","Epoch 4/5\n","236/236 [==============================] - 1s 2ms/step - loss: 1.2999 - accuracy: 0.3957 - val_loss: 1.0504 - val_accuracy: 0.4055\n","Epoch 5/5\n","236/236 [==============================] - 1s 2ms/step - loss: 1.2017 - accuracy: 0.4024 - val_loss: 1.0471 - val_accuracy: 0.4082\n","Generating test predictions...\n","[[0.39162531 0.21703912 0.39133558]\n"," [0.39162531 0.21703912 0.39133558]\n"," [0.39162531 0.21703912 0.39133558]\n"," ...\n"," [0.39162531 0.21703912 0.39133558]\n"," [0.39162531 0.21703912 0.39133558]\n"," [0.39162531 0.21703912 0.39133558]]\n","74/74 - 0s - loss: 1.0529 - accuracy: 0.4102 - 86ms/epoch - 1ms/step\n","Score: 1.05\n","Validation Accuracy: 0.41\n"]}],"source":["\n","print(device_lib.list_local_devices())\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","\n","# Deep Neural Network \"MLP\": multi layer Perceptron\n","#with tf.device('/GPU:0'):\n","model = Sequential()\n","\n","\n","model.add(Dense(256, input_dim=X_train.shape[1]))\n","\n","# 0.42 accuracy.\n","model.add(Activation('relu'))\n","model.add(Dropout(0.4))\n","model.add(Dense(128))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(3))\n","model.add(Activation('softmax'))\n","\n","# we'll use categorical xent for the loss, and RMSprop as the optimizer\n","model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n","\n","print(\"Training...\")\n","model.fit(X_train, Y_train, epochs=5, batch_size=32, validation_split=0.2)\n","\n","print(\"Generating test predictions...\")\n","preds = model.predict(X_test, verbose=0)\n","print(preds)\n","\n","# Evaluating the model\n","score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = 32)\n","print(\"Score: %.2f\" % (score))\n","print(\"Validation Accuracy: %.2f\" % (acc))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are using device name \"cuda\"\n"]}],"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(f'We are using device name \"{device}\"')"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":363,"status":"ok","timestamp":1665816049410,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"VUc6d02pPnfi","outputId":"528f0578-d32b-4313-d541-110b750d95b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["74/74 [==============================] - 0s 699us/step\n"]}],"source":["predict = model.predict(X_test)\n","preds = predict\n","p = preds"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":493,"status":"error","timestamp":1665816052127,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"SCDeuT03Ppfg","outputId":"960a3941-59fb-4162-cd46-eaa2cbfd3765"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.22710698282227437\n","0.4101910828025478\n","0.24364117397481594\n","0.39406684532353814\n","0.4123142250530786\n","0.24784210492556172\n","0.6516020032365524\n","0.8072186836518047\n","0.7211102996344364\n","0.42346070887324794\n","0.6008492569002123\n","0.45806654887263404\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n","#print(tmpMat.shape)\n","for i in range(0,len(predict)):\n","    if(p[i][0] > p[i][1] and p[i][0] > p[i][2]):\n","        tmpMat[i][0]=1\n","    elif(p[i][1] > p[i][0] and p[i][1] > p[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","\n","test_y = Y_test.to_numpy()\n","\n","\n","\n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))  \n","\n","\n","\n","# OF CAG\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","\n","# Of class NAG\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","# Of class OAG\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1665816067483,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"B_KshrOfHNhH"},"outputs":[],"source":["#CNN\n","# CNN: ConvNeuralNets\n","\n","nb_filter = 250\n","filter_length = 3\n","hidden_dims = 250\n","nb_epoch = 2"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1078,"status":"ok","timestamp":1665816071960,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"J_TZyIBeHV5z","outputId":"a692dfda-e412-4cd7-b452-b3a48190a955"},"outputs":[{"name":"stdout","output_type":"stream","text":["Build model...\n"]}],"source":["from keras.layers.convolutional import Convolution1D\n","from keras import backend as K\n","\n","print('Build model...')\n","model = Sequential()\n","model.add(Embedding(4000, 128))\n","# we add a Convolution1D, which will learn nb_filter\n","# word group filters of size filter_length:\n","model.add(Convolution1D(4000, 128, \n","                        activation='relu'))\n","\n","def max_1d(X_train):\n","    return K.max(X_train, axis=1)\n","\n","model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n","model.add(Dense(hidden_dims)) \n","model.add(Dropout(0.2)) \n","model.add(Activation('relu'))\n","model.add(Dense(3))\n","model.add(Activation('sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNmO92j-Hin_","outputId":"d3287bda-97d9-4b23-aa2d-60575b95627b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train...\n","Epoch 1/2\n","76/76 [==============================] - 209s 2s/step - loss: 0.6491 - accuracy: 0.4028 - val_loss: 0.5844 - val_accuracy: 0.4395\n","Epoch 2/2\n","76/76 [==============================] - 98s 1s/step - loss: 0.5256 - accuracy: 0.5751 - val_loss: 0.5267 - val_accuracy: 0.5801\n","74/74 [==============================] - 8s 100ms/step - loss: 0.5090 - accuracy: 0.6089\n","Test score: 0.5089619159698486\n","Test accuracy: 0.6089171767234802\n"]}],"source":["print('Train...')\n","model.fit(X_train, Y_train, batch_size=100, epochs=2,\n","          validation_split=0.2)\n","score, acc = model.evaluate(X_test, Y_test, batch_size=32)\n","print('Test score:', score)\n","print('Test accuracy:', acc)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"k-tugF4iQXjc"},"outputs":[{"name":"stdout","output_type":"stream","text":["74/74 [==============================] - 7s 93ms/step\n","0.6141793967773401\n","0.6089171974522293\n","0.6088042268804645\n","0.6667732985186561\n","0.6564755838641189\n","0.6588718178408396\n","0.8335531591204847\n","0.8373673036093419\n","0.8353000622238581\n","0.7198157968214506\n","0.7239915074309978\n","0.7198117106650845\n"]}],"source":["predict = model.predict(X_test)\n","test_y = Y_test.to_numpy()\n","tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n","\n","for i in range(0,len(predict)):\n","    if(predict[i][0] > predict[i][1] and predict[i][0] > predict[i][2]):\n","        tmpMat[i][0]=1\n","    elif(predict[i][1] > predict[i][0] and predict[i][1] > predict[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","        \n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))\n","\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"EB6CYs8PQNs-"},"source":["## LSTM"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"qhPwhGFqQQwd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 387, 128)          512000    \n","                                                                 \n"," lstm (LSTM)                 (None, 196)               254800    \n","                                                                 \n"," dense_5 (Dense)             (None, 3)                 591       \n","                                                                 \n","=================================================================\n","Total params: 767,391\n","Trainable params: 767,391\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["# LSTM Model for prediction\n","embed_dim = 128\n","lstm_out = 196\n","batch_size = 100\n","\n","model = Sequential()\n","#model.add(Embedding(4000, embed_dim,input_length = X.shape[1], dropout = 0.2))\n","model.add(Embedding(4000, embed_dim,input_length = X.shape[1]))\n","#model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n","model.add(LSTM(lstm_out))\n","model.add(Dense(3,activation='softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","print(model.summary())"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"_hSqeA77QrI1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","95/95 - 3s - loss: 0.9749 - accuracy: 0.4779 - 3s/epoch - 36ms/step\n","Epoch 2/2\n","95/95 - 2s - loss: 0.7817 - accuracy: 0.6282 - 2s/epoch - 22ms/step\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x2216d8d2b50>"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(X_train, Y_train, batch_size = batch_size, epochs = 2, verbose = 2)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"_Z7MmCfNQtRB"},"outputs":[{"name":"stdout","output_type":"stream","text":["24/24 - 0s - loss: 0.8752 - accuracy: 0.5843 - 416ms/epoch - 17ms/step\n","Score: 0.88\n","Validation Accuracy: 0.58\n"]}],"source":["score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n","print(\"Score: %.2f\" % (score))\n","print(\"Validation Accuracy: %.2f\" % (acc))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"a6iyQPUAQvyV"},"outputs":[{"name":"stdout","output_type":"stream","text":["74/74 [==============================] - 1s 8ms/step\n"]}],"source":["p = model.predict(X_test)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"dQu2GzEPQyLW"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5857612574076241\n","0.5842887473460722\n","0.5845062839437897\n","0.6427934391732812\n","0.6386411889596603\n","0.6401736064155765\n","0.8256149751646873\n","0.8280254777070064\n","0.8267676338085272\n","0.6993076587617474\n","0.7019108280254777\n","0.7002618165366228\n"]}],"source":["tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n","\n","for i in range(0,len(predict)):\n","    if(p[i][0] > p[i][1] and p[i][0] > p[i][2]):\n","        tmpMat[i][0]=1\n","    elif(p[i][1] > p[i][0] and p[i][1] > p[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","\n","test_y = Y_test.to_numpy()\n","\n","# For LSTM model\n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))\n","\n","# Of Class CAG\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","# Of class NAG\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","# Of class OAG\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"ACpaROBsRBJv"},"source":["## Naive Bayes"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"8x1p8S5mQ-iV"},"outputs":[{"name":"stdout","output_type":"stream","text":["- CE : 3.54, KL : 0.47\n"]}],"source":["import numpy as np\n","from sklearn import datasets\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchbnn as bnn\n","\n","model = nn.Sequential(\n","    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=X_train.shape[1], out_features=4000),\n","    nn.ReLU(),\n","    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=4000, out_features=3),\n",")\n","\n","model = model.to(device)\n","\n","\n","ce_loss = nn.CrossEntropyLoss()\n","kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n","kl_weight = 0.01\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","kl_weight = 0.1\n","\n","for step in range(3000):\n","    pre = model(X_train_tensor)\n","    ce = ce_loss(pre, Y_train_tensor)\n","    kl = kl_loss(model)\n","    cost = ce + kl_weight*kl\n","    \n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","    \n","_, predicted = torch.max(pre.data, 1)\n","total = Y_train_tensor.size(0)\n","#correct = (predicted == Y_train_tensor).sum()\n","#print('- Accuracy: %f %%' % (100 * float(correct) / total))\n","print('- CE : %2.2f, KL : %2.2f' % (ce.item(), kl.item()))\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["model.eval()\n","predict = model(X_test_tensor)\n","preds = predict\n","p = preds"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.4150505020856367\n","0.4220806794055202\n","0.3529458830358359\n","0.5447077314024311\n","0.4653927813163482\n","0.41799338117676954\n","0.7518767875744102\n","0.7987261146496816\n","0.7594931836701142\n","0.5225647022187301\n","0.5800424628450106\n","0.5097267398257043\n"]}],"source":["tmpMat = np.zeros((len(Y_test_tensor), 3), dtype=int)\n","#print(tmpMat.shape)\n","for i in range(0,len(predict)):\n","    if(p[i][0] > p[i][1] and p[i][0] > p[i][2]):\n","        tmpMat[i][0]=1\n","    elif(p[i][1] > p[i][0] and p[i][1] > p[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","\n","\n","Y_test_tensor = Y_test_tensor.cpu().detach()\n","test_y = Y_test_tensor.numpy()\n","\n","\n","\n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))  \n","\n","\n","\n","# OF CAG\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","\n","# Of class NAG\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","# Of class OAG\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"_kkKQo9mRKgS"},"source":["## SVM"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(8202, 2)\n","(3586, 2)\n","(8202, 4302)\n"]}],"source":["#Import svm model\n","from sklearn import svm\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Decide on a percentage split 70/30\n","msk = np.random.rand(len(df_pretoken)) < 0.7\n","train_df = df_pretoken[msk]\n","test_df = df_pretoken[~msk]\n","\n","print(train_df.shape)\n","print(test_df.shape)\n","\n","#print(train_df.head(10))\n","\n","vectorizer = TfidfVectorizer(min_df=5, max_df=0.8, \n","                            sublinear_tf=True, use_idf=True)\n","train_features = vectorizer.fit_transform(train_df['post'])\n","test_features = vectorizer.transform(test_df['post'])\n","print(train_features.shape)\n","\n","#Create a svm Classifier\n","model = svm.SVC(kernel='linear') # Linear Kernel\n","\n","#Train the model using the training sets\n","model.fit(train_features, train_df['label'])\n","\n","#Predict the response for test dataset\n","predict = model.predict(test_features)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["preds = predict\n","p = preds"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.5755716675962075\n"]}],"source":["#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics\n","\n","# Model Accuracy: how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(test_df['label'],predict))"]},{"cell_type":"markdown","metadata":{"id":"obkby72nRUXY"},"source":["## Transfer Learning"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"8383SyYqTL9r"},"outputs":[{"name":"stderr","output_type":"stream","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"name":"stdout","output_type":"stream","text":["Moving 0 files to the new cache system\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4bf60af05514e44ad4717b51926197a","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['sop_classifier.classifier.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'sop_classifier.classifier.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoModel, AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n","model = AutoModel.from_pretrained('ai4bharat/indic-bert')"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AlbertModel(\n","  (embeddings): AlbertEmbeddings(\n","    (word_embeddings): Embedding(4000, 128, padding_idx=0)\n","    (position_embeddings): Embedding(512, 128)\n","    (token_type_embeddings): Embedding(2, 128)\n","    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0, inplace=False)\n","  )\n","  (encoder): AlbertTransformer(\n","    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n","    (albert_layer_groups): ModuleList(\n","      (0): AlbertLayerGroup(\n","        (albert_layers): ModuleList(\n","          (0): AlbertLayer(\n","            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (attention): AlbertAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (attention_dropout): Dropout(p=0, inplace=False)\n","              (output_dropout): Dropout(p=0, inplace=False)\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n","            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (pooler): Linear(in_features=768, out_features=3, bias=True)\n","  (pooler_activation): Tanh()\n",")\n"]}],"source":["model.embeddings.word_embeddings = nn.Embedding(4000,128,padding_idx=0)\n","model.pooler = nn.Linear(in_features = 768, out_features=3)\n","\n","model.train()\n","\n","print(model)\n","\n","#model.train()\n","\n","#model(X_test)\n","\n","\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOeXCBAyBTSGoUfgw1A6UT6","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"5152cc3a395a70098988def549eacca6eb731ce4ed08ed350a66e5f78a6f87ea"}}},"nbformat":4,"nbformat_minor":0}
