{"cells":[{"cell_type":"markdown","metadata":{"id":"-atAFiq8kLtz"},"source":["# Agressive Social Media Text Processing\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cuSKhlIZe49F"},"source":["## Project Main code\n","1. Read in the dataset\n","1. Pre-process Dataset\n","1. Create Labels for classification\n","1. Build Model 1\n","1. Train Model 1\n","1. Display Accuracy\n","1. Build Model 2\n","1. Train Model 2\n","1. Display Accuracy\n","1. Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"HLl6PmlAlR7f"},"source":["## Prepare Google Drive and Filename"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11875,"status":"ok","timestamp":1665815966980,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"zpaXbQQo6YNu","outputId":"69918163-cacd-420e-89fe-710c8ac56f57"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["#All library imports are here\n","import numpy as np\n","import pandas as pd\n","import re\n","import string\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import keras\n","import tensorflow as tf\n","import tensorflow.python.keras.backend as K\n","from tensorflow.python.client import device_lib\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","import re\n","from keras.layers.core import Dense, Dropout, Activation, Lambda\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are using device name \"cuda\"\n"]}],"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(f'We are using device name \"{device}\"')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":152,"status":"ok","timestamp":1665811225937,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"miF_Qnvmlhrb"},"outputs":[],"source":["dataset_file = 'D:/project/ece570nlp/dataSet.csv'"]},{"cell_type":"markdown","metadata":{"id":"5mvHbC0Jk8_D"},"source":["## Read Dataset\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1665284777040,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"vYXpwD2Eez6l","outputId":"dfe51077-a431-4d07-bd21-59b3f95cc7c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["(12000, 3)\n"]}],"source":["#In this section we Read the dataset from the Paper\n","\n","# Data is stored in csv format\n","\n","# Load CSV using Pandas\n","\n","filename = dataset_file\n","names = ['id', 'post', 'label']\n","df_original = pd.read_csv(filename, names=names, encoding='UTF-8')\n","print(df_original.shape)"]},{"cell_type":"markdown","metadata":{"id":"PTE7O55mnEiG"},"source":["## Print the first 10 lines of the data as an example"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1665284780853,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"jk24zLC2nNDj","outputId":"fefa5b28-e879-4b1e-efb1-07b400dc032b"},"outputs":[{"name":"stdout","output_type":"stream","text":["                            id  \\\n","0   facebook_corpus_msr_401470   \n","1   facebook_corpus_msr_386695   \n","2   facebook_corpus_msr_373389   \n","3   facebook_corpus_msr_917635   \n","4   facebook_corpus_msr_382517   \n","5   facebook_corpus_msr_403274   \n","6  facebook_corpus_msr_1723083   \n","7   facebook_corpus_msr_325257   \n","8    facebook_corpus_msr_23447   \n","9  facebook_corpus_msr_1477104   \n","\n","                                                post label  \n","0  Mahmood Ghaznavi Aor ABdali ko bhol gaya ha tu...   OAG  \n","1  Bhai 60sal pehle desh me kya tha pehle pta kro...   CAG  \n","2  chutiya friday ko isliye releae krte kyoki wee...   CAG  \n","3                                         जय मोदीराज   CAG  \n","4     UPA walo ne bahot kuch kr diya tha desh k liye   CAG  \n","5  Pan ko Aadhar se link ki zarurat kuy hai? Supr...   CAG  \n","6  काकर पाथर जोड़ के मस्जिद लई बनाय।\\n\\nता चढ़ि मुल...   OAG  \n","7  Us raat tere papa k jageh mera sperm gya tha u...   OAG  \n","8                                       गटर के कीड़े   OAG  \n","9  Waise bandhu jet lag se bachne ke liye Raat ko...   NAG  \n"]}],"source":["first_10_rows = df_original.head(10)\n","print(first_10_rows)"]},{"cell_type":"markdown","metadata":{"id":"4X0XWT9l0Izc"},"source":["## Read the processed data back to a dataframe"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"uPehzMc00DsK"},"outputs":[],"source":["dataset_processed_file = 'D:/project/ece570nlp/DataSetProcessed.csv'"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1665285067682,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"FNmIvXYv0RQA","outputId":"98735916-7fef-457f-96c3-ba3802e8b0a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["(11999, 3)\n","                             id  \\\n","0   facebook_corpus_msr_401470    \n","1   facebook_corpus_msr_386695    \n","2   facebook_corpus_msr_373389    \n","3   facebook_corpus_msr_917635    \n","4   facebook_corpus_msr_382517    \n","5   facebook_corpus_msr_403274    \n","6  facebook_corpus_msr_1723083    \n","7   facebook_corpus_msr_325257    \n","8    facebook_corpus_msr_23447    \n","9  facebook_corpus_msr_1477104    \n","\n","                                                post label  \n","0   Mahmood Ghaznavi Aor ABdali ko bhol gaya ha t...   OAG  \n","1   Bhai 60sal pehle desh me kya tha pehle pta kr...   CAG  \n","2   chutiya friday ko isliye releae krte kyoki we...   CAG  \n","3                                       jai modiraj    CAG  \n","4    UPA walo ne bahot kuch kr diya tha desh k liye    CAG  \n","5   Pan ko Aadhar se link ki zarurat kuy hai? Sup...   CAG  \n","6   kaakar pathar jod ke masjid lai banaay. ta ch...   OAG  \n","7   Us raat tere papa k jageh mera sperm gya tha ...   OAG  \n","8                                    gater ke keede    OAG  \n","9   Waise bandhu jet lag se bachne ke liye Raat k...   NAG  \n"]}],"source":["filename = dataset_processed_file\n","header = ['id', 'post', 'label']\n","df_processed = pd.read_csv(filename, names=header)\n","print(df_processed.shape)\n","print(df_processed.head(10))"]},{"cell_type":"markdown","metadata":{"id":"7oIMgR742nGu"},"source":["## Split the dataset into training and test data"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1665285073835,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"w7RKoRuq2wdl","outputId":"d84c8185-8a9b-4b5b-9055-5c11d385c39b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(11999, 2)\n"]}],"source":["df_processed = df_processed.iloc[: , 1:]  # Drop first column\n","print(df_processed.shape)\n","\n","# Save without ID to csv file\n","file_no_id = 'D:/project/ece570nlp/fileNoID.csv'\n","df_processed.to_csv(file_no_id, index=False, header=False)\n","\n","# Decide on a percentage split 70/30\n","#msk = np.random.rand(len(df)) < 0.7\n","#train_df = df_processed[msk]\n","#test_df = df_processed[~msk]\n","\n","#print(train_df.shape)\n","#print(test_df.shape)\n","\n","#print(train_df.head(10))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["file_no_id = 'D:/project/ece570nlp/fileNoID.csv'\n","data_cleanedup = 'D:/project/ece570nlp/datacleanedup.csv'"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(11788, 2)\n","(11788,)\n","(11788,)\n","22.9778236204229\n","11773\n","11773\n","(11773, 387)\n","(11773, 3)\n","(9418, 387)\n","(9418, 3)\n","(2355, 387)\n","(2355, 3)\n"]}],"source":["header = ['post', 'label']\n","df_pretoken = pd.read_csv(data_cleanedup, header=None, names=header)\n","print(df_pretoken.shape)\n","\n","X_pretoken = df_pretoken['post']\n","Y_pretoken = df_pretoken['label']\n","\n","print(X_pretoken.shape)\n","print(Y_pretoken.shape)\n","\n","tokenizer = Tokenizer(num_words=4000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n","tokenizer.fit_on_texts(X_pretoken)\n","#print(tokenizer.word_index)\n","\n","X = tokenizer.texts_to_sequences(X_pretoken)\n","#X = word_tokenize(str(X_pretoken))  # Using NLTK\n","#print(X)\n","\n","\n","sum_tot = 0\n","for seq in X:\n","    sum_tot += len(seq)\n","\n","print(float(sum_tot/11634.0))\n","\n","new_x = []\n","new_y = []\n","\n","for i in range(0, len(X)):\n","    if len(X[i]) < 400:\n","        new_x.append(X[i])\n","        new_y.append(Y_pretoken[i])\n","\n","print(len(new_x))\n","print(len(new_y))\n","\n","X = pad_sequences(new_x)\n","Y = pd.get_dummies(new_y)\n","\n","print(X.shape)\n","print(Y.shape)\n","\n","#print(X)\n","#print(Y)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n","\n","print(X_train.shape)\n","print(Y_train.shape)\n","print(X_test.shape)\n","print(Y_test.shape)\n","\n","\n","X_train_tensor = torch.Tensor(X_train)\n","Y_train_tensor = torch.Tensor(np.array(Y_train))\n","\n","X_train_tensor = X_train_tensor.to(device)\n","Y_train_tensor = Y_train_tensor.to(device)\n","\n","X_test_tensor = torch.Tensor(X_test)\n","Y_test_tensor = torch.Tensor(np.array(Y_test))\n","\n","X_test_tensor = X_test_tensor.to(device)\n","Y_test_tensor = Y_test_tensor.to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"jhr0C61ariHV"},"source":["Build the MLP Network"]},{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7663,"status":"ok","timestamp":1665816043228,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"JOxsNpLvKVmA","outputId":"3e339604-f3ca-426b-a2eb-2351c239c69f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 17661871645166738761\n","xla_global_id: -1\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 10064232448\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 9159235520282909350\n","physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n","xla_global_id: 416903419\n","]\n","Num GPUs Available:  1\n","Training...\n","Epoch 1/5\n","236/236 [==============================] - 1s 5ms/step - loss: 54.5392 - accuracy: 0.3768 - val_loss: 2.3858 - val_accuracy: 0.3758\n","Epoch 2/5\n","236/236 [==============================] - 1s 3ms/step - loss: 2.8351 - accuracy: 0.4012 - val_loss: 1.0985 - val_accuracy: 0.3960\n","Epoch 3/5\n","236/236 [==============================] - 1s 3ms/step - loss: 1.4474 - accuracy: 0.3989 - val_loss: 1.0584 - val_accuracy: 0.4076\n","Epoch 4/5\n","236/236 [==============================] - 1s 3ms/step - loss: 1.2348 - accuracy: 0.4101 - val_loss: 1.0503 - val_accuracy: 0.4071\n","Epoch 5/5\n","236/236 [==============================] - 1s 3ms/step - loss: 1.1843 - accuracy: 0.4002 - val_loss: 1.0466 - val_accuracy: 0.4066\n","Generating test predictions...\n","[[0.39048278 0.21392675 0.39559054]\n"," [0.39048278 0.21392675 0.39559054]\n"," [0.39048278 0.21392675 0.39559054]\n"," ...\n"," [0.39048278 0.21392675 0.39559054]\n"," [0.39048278 0.21392675 0.39559054]\n"," [0.39048278 0.21392675 0.39559054]]\n","74/74 - 0s - loss: 1.0507 - accuracy: 0.3932 - 99ms/epoch - 1ms/step\n","Score: 1.05\n","Validation Accuracy: 0.39\n"]}],"source":["\n","print(device_lib.list_local_devices())\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","\n","# Deep Neural Network \"MLP\": multi layer Perceptron\n","#with tf.device('/GPU:0'):\n","model = Sequential()\n","\n","\n","model.add(Dense(256, input_dim=X_train.shape[1]))\n","\n","# 0.42 accuracy.\n","model.add(Activation('relu'))\n","model.add(Dropout(0.4))\n","model.add(Dense(128))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(3))\n","model.add(Activation('softmax'))\n","\n","# we'll use categorical xent for the loss, and RMSprop as the optimizer\n","model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n","\n","print(\"Training...\")\n","model.fit(X_train, Y_train, epochs=5, batch_size=32, validation_split=0.2)\n","\n","print(\"Generating test predictions...\")\n","preds = model.predict(X_test, verbose=0)\n","print(preds)\n","\n","# Evaluating the model\n","score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = 32)\n","print(\"Score: %.2f\" % (score))\n","print(\"Validation Accuracy: %.2f\" % (acc))"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are using device name \"cuda\"\n"]}],"source":["device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(f'We are using device name \"{device}\"')"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":363,"status":"ok","timestamp":1665816049410,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"VUc6d02pPnfi","outputId":"528f0578-d32b-4313-d541-110b750d95b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["74/74 [==============================] - 0s 2ms/step\n"]}],"source":["predict = model.predict(X_test)\n","preds = predict\n","p = preds"]},{"cell_type":"code","execution_count":84,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":493,"status":"error","timestamp":1665816052127,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"SCDeuT03Ppfg","outputId":"960a3941-59fb-4162-cd46-eaa2cbfd3765"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5687800680776453\n","0.3932059447983015\n","0.22245419284854573\n","0.7574666240955901\n","0.5859872611464968\n","0.4334714096105611\n","0.6516020032365524\n","0.8072186836518047\n","0.7211102996344364\n","0.7615613844258406\n","0.3932059447983015\n","0.22245446862725818\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n","#print(tmpMat.shape)\n","for i in range(0,len(predict)):\n","    if(p[i][0] > p[i][1] and p[i][0] > p[i][2]):\n","        tmpMat[i][0]=1\n","    elif(p[i][1] > p[i][0] and p[i][1] > p[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","\n","test_y = Y_test.to_numpy()\n","\n","\n","\n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))  \n","\n","\n","\n","# OF CAG\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","\n","# Of class NAG\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","# Of class OAG\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"code","execution_count":85,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1665816067483,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"B_KshrOfHNhH"},"outputs":[],"source":["#CNN\n","# CNN: ConvNeuralNets\n","\n","nb_filter = 250\n","filter_length = 3\n","hidden_dims = 250\n","nb_epoch = 2"]},{"cell_type":"code","execution_count":86,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1078,"status":"ok","timestamp":1665816071960,"user":{"displayName":"Nikhil Krishna","userId":"13931694112886394930"},"user_tz":420},"id":"J_TZyIBeHV5z","outputId":"a692dfda-e412-4cd7-b452-b3a48190a955"},"outputs":[{"name":"stdout","output_type":"stream","text":["Build model...\n"]}],"source":["from keras.layers.convolutional import Convolution1D\n","from keras import backend as K\n","\n","print('Build model...')\n","model = Sequential()\n","model.add(Embedding(4000, 128))\n","# we add a Convolution1D, which will learn nb_filter\n","# word group filters of size filter_length:\n","model.add(Convolution1D(4000, 128, \n","                        activation='relu'))\n","\n","def max_1d(X_train):\n","    return K.max(X_train, axis=1)\n","\n","model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n","model.add(Dense(hidden_dims)) \n","model.add(Dropout(0.2)) \n","model.add(Activation('relu'))\n","model.add(Dense(3))\n","model.add(Activation('sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNmO92j-Hin_","outputId":"d3287bda-97d9-4b23-aa2d-60575b95627b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train...\n","Epoch 1/2\n","76/76 [==============================] - 98s 1s/step - loss: 0.6338 - accuracy: 0.4197 - val_loss: 0.5758 - val_accuracy: 0.4910\n","Epoch 2/2\n","76/76 [==============================] - 99s 1s/step - loss: 0.5139 - accuracy: 0.5798 - val_loss: 0.5521 - val_accuracy: 0.5441\n","74/74 [==============================] - 7s 95ms/step - loss: 0.5356 - accuracy: 0.5686\n","Test score: 0.5356228947639465\n","Test accuracy: 0.5685774683952332\n"]}],"source":["print('Train...')\n","model.fit(X_train, Y_train, batch_size=100, epochs=2,\n","          validation_split=0.2)\n","score, acc = model.evaluate(X_test, Y_test, batch_size=32)\n","print('Test score:', score)\n","print('Test accuracy:', acc)"]},{"cell_type":"code","execution_count":88,"metadata":{"id":"k-tugF4iQXjc"},"outputs":[{"name":"stdout","output_type":"stream","text":["74/74 [==============================] - 7s 96ms/step\n","0.583991416883302\n","0.5685774946921444\n","0.567302978671823\n","0.6395874598298449\n","0.6467091295116772\n","0.6370797878408296\n","0.8299289904054777\n","0.7804670912951168\n","0.7964800072600619\n","0.7085845735746974\n","0.7099787685774946\n","0.7091872306377996\n"]}],"source":["predict = model.predict(X_test)\n","test_y = Y_test.to_numpy()\n","tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n","\n","for i in range(0,len(predict)):\n","    if(predict[i][0] > predict[i][1] and predict[i][0] > predict[i][2]):\n","        tmpMat[i][0]=1\n","    elif(predict[i][1] > predict[i][0] and predict[i][1] > predict[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","        \n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))\n","\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"EB6CYs8PQNs-"},"source":["## LSTM"]},{"cell_type":"code","execution_count":89,"metadata":{"id":"qhPwhGFqQQwd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_4 (Embedding)     (None, 387, 128)          512000    \n","                                                                 \n"," lstm_1 (LSTM)               (None, 196)               254800    \n","                                                                 \n"," dense_11 (Dense)            (None, 3)                 591       \n","                                                                 \n","=================================================================\n","Total params: 767,391\n","Trainable params: 767,391\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["# LSTM Model for prediction\n","embed_dim = 128\n","lstm_out = 196\n","batch_size = 100\n","\n","model = Sequential()\n","#model.add(Embedding(4000, embed_dim,input_length = X.shape[1], dropout = 0.2))\n","model.add(Embedding(4000, embed_dim,input_length = X.shape[1]))\n","#model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n","model.add(LSTM(lstm_out))\n","model.add(Dense(3,activation='softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","print(model.summary())"]},{"cell_type":"code","execution_count":90,"metadata":{"id":"_hSqeA77QrI1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","95/95 - 3s - loss: 0.9891 - accuracy: 0.4676 - 3s/epoch - 34ms/step\n","Epoch 2/2\n","95/95 - 2s - loss: 0.7915 - accuracy: 0.6241 - 2s/epoch - 23ms/step\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x23bb9a8c0a0>"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(X_train, Y_train, batch_size = batch_size, epochs = 2, verbose = 2)"]},{"cell_type":"code","execution_count":91,"metadata":{"id":"_Z7MmCfNQtRB"},"outputs":[{"name":"stdout","output_type":"stream","text":["24/24 - 1s - loss: 0.8875 - accuracy: 0.5758 - 636ms/epoch - 27ms/step\n","Score: 0.89\n","Validation Accuracy: 0.58\n"]}],"source":["score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n","print(\"Score: %.2f\" % (score))\n","print(\"Validation Accuracy: %.2f\" % (acc))"]},{"cell_type":"code","execution_count":92,"metadata":{"id":"a6iyQPUAQvyV"},"outputs":[{"name":"stdout","output_type":"stream","text":["74/74 [==============================] - 1s 12ms/step\n"]}],"source":["p = model.predict(X_test)"]},{"cell_type":"code","execution_count":93,"metadata":{"id":"dQu2GzEPQyLW"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5768342976472942\n","0.575796178343949\n","0.5758490683304347\n","0.640354432486374\n","0.6365180467091295\n","0.6379700727536683\n","0.8327301431119528\n","0.8339702760084926\n","0.8333352506185554\n","0.6781001808602658\n","0.681104033970276\n","0.6792268936856631\n"]}],"source":["tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n","\n","for i in range(0,len(predict)):\n","    if(p[i][0] > p[i][1] and p[i][0] > p[i][2]):\n","        tmpMat[i][0]=1\n","    elif(p[i][1] > p[i][0] and p[i][1] > p[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","\n","test_y = Y_test.to_numpy()\n","\n","# For LSTM model\n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))\n","\n","# Of Class CAG\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","# Of class NAG\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","# Of class OAG\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"ACpaROBsRBJv"},"source":["## Naive Bayes"]},{"cell_type":"code","execution_count":94,"metadata":{"id":"8x1p8S5mQ-iV"},"outputs":[{"name":"stdout","output_type":"stream","text":["- CE : 4.88, KL : 0.53\n"]}],"source":["import numpy as np\n","from sklearn import datasets\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchbnn as bnn\n","\n","model = nn.Sequential(\n","    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=X_train.shape[1], out_features=4000),\n","    nn.ReLU(),\n","    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=4000, out_features=3),\n",")\n","\n","model = model.to(device)\n","\n","\n","ce_loss = nn.CrossEntropyLoss()\n","kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n","kl_weight = 0.01\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","kl_weight = 0.1\n","\n","for step in range(3000):\n","    pre = model(X_train_tensor)\n","    ce = ce_loss(pre, Y_train_tensor)\n","    kl = kl_loss(model)\n","    cost = ce + kl_weight*kl\n","    \n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","    \n","_, predicted = torch.max(pre.data, 1)\n","total = Y_train_tensor.size(0)\n","#correct = (predicted == Y_train_tensor).sum()\n","#print('- Accuracy: %f %%' % (100 * float(correct) / total))\n","print('- CE : %2.2f, KL : %2.2f' % (ce.item(), kl.item()))\n"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["model.eval()\n","predict = model(X_test_tensor)\n","preds = predict\n","p = preds"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.3772226905014979\n","0.4029723991507431\n","0.33871299862699755\n","0.5272289936818448\n","0.4607218683651805\n","0.42212093200596995\n","0.7021776443696057\n","0.7681528662420383\n","0.7258461097138258\n","0.5254370648612072\n","0.5770700636942675\n","0.5165315522121082\n"]}],"source":["tmpMat = np.zeros((len(Y_test_tensor), 3), dtype=int)\n","#print(tmpMat.shape)\n","for i in range(0,len(predict)):\n","    if(p[i][0] > p[i][1] and p[i][0] > p[i][2]):\n","        tmpMat[i][0]=1\n","    elif(p[i][1] > p[i][0] and p[i][1] > p[i][2]):\n","        tmpMat[i][1]=1\n","    else:\n","        tmpMat[i][2]=1\n","\n","\n","Y_test_tensor = Y_test_tensor.cpu().detach()\n","test_y = Y_test_tensor.numpy()\n","\n","\n","\n","print(precision_score(test_y, tmpMat, average='weighted'))\n","print(recall_score(test_y, tmpMat, average='weighted'))\n","print(f1_score(test_y, tmpMat, average='weighted'))  \n","\n","\n","\n","# OF CAG\n","print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n","\n","\n","# Of class NAG\n","print(precision_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(recall_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","print(f1_score(test_y[:,1], tmpMat[:,1], average='weighted'))\n","\n","# Of class OAG\n","print(precision_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(recall_score(test_y[:,2], tmpMat[:,2], average='weighted'))\n","print(f1_score(test_y[:,2], tmpMat[:,2], average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"_kkKQo9mRKgS"},"source":["## SVM"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [9418, 28254]","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32md:\\project\\ece570nlp\\project_main_code.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39mSVC(kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# Linear Kernel\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#Train the model using the training sets\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train,Y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#Predict the response for test dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m predict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    191\u001b[0m         X,\n\u001b[0;32m    192\u001b[0m         y,\n\u001b[0;32m    193\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    194\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    195\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    196\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    199\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    201\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[0;32m    202\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[0;32m    203\u001b[0m )\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[1;32m--> 981\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m    983\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n","\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9418, 28254]"]}],"source":["#Import svm model\n","from sklearn import svm\n","\n","#Create a svm Classifier\n","model = svm.SVC(kernel='linear') # Linear Kernel\n","\n","#Train the model using the training sets\n","model.fit(X_train,)\n","\n","#Predict the response for test dataset\n","predict = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics\n","\n","# Model Accuracy: how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"obkby72nRUXY"},"source":["## Transfer Learning"]},{"cell_type":"code","execution_count":120,"metadata":{"id":"8383SyYqTL9r"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b9b94148a954bfb9ae20dfe16f6dd5f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/507 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nikhi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86f5a8a9c85f4ea8b5f0d61fa7eb4322","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e0a18aaac0645068893911421782c7c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/135M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['sop_classifier.classifier.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'sop_classifier.classifier.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoModel, AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n","model = AutoModel.from_pretrained('ai4bharat/indic-bert')"]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AlbertModel(\n","  (embeddings): AlbertEmbeddings(\n","    (word_embeddings): Embedding(4000, 128, padding_idx=0)\n","    (position_embeddings): Embedding(512, 128)\n","    (token_type_embeddings): Embedding(2, 128)\n","    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0, inplace=False)\n","  )\n","  (encoder): AlbertTransformer(\n","    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n","    (albert_layer_groups): ModuleList(\n","      (0): AlbertLayerGroup(\n","        (albert_layers): ModuleList(\n","          (0): AlbertLayer(\n","            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (attention): AlbertAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (attention_dropout): Dropout(p=0, inplace=False)\n","              (output_dropout): Dropout(p=0, inplace=False)\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n","            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (pooler): Linear(in_features=768, out_features=3, bias=True)\n","  (pooler_activation): Tanh()\n","  (fc): Linear(in_features=768, out_features=3, bias=True)\n",")\n"]},{"ename":"TypeError","evalue":"'int' object is not callable","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\project\\ece570nlp\\project_main_code.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X61sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X61sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/project/ece570nlp/project_main_code.ipynb#X61sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model(X_test)\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:706\u001b[0m, in \u001b[0;36mAlbertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    705\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[0;32m    707\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    708\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n","\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"]}],"source":["model.embeddings.word_embeddings = nn.Embedding(4000,128,padding_idx=0)\n","model.pooler = nn.Linear(in_features = 768, out_features=3)\n","\n","print(model)\n","\n","model.train()\n","\n","model(X_test)\n","\n","\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOeXCBAyBTSGoUfgw1A6UT6","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"5152cc3a395a70098988def549eacca6eb731ce4ed08ed350a66e5f78a6f87ea"}}},"nbformat":4,"nbformat_minor":0}
